
\documentclass[paper=a4]{ctexart} % A4 paper and 11pt font size
\usepackage{fontspec,caption, xeCJK,amsmath,tocvsec2,extarrows, chngpage, adjustbox, clrscode}
\usepackage{listings}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm,amssymb, enumerate} % Math packages
\usepackage{framed,floatflt}   
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{picins}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
% \allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{wrapfig}
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\lstset{
        keywordstyle=\color{blue}, %设置关键字颜色
        commentstyle=\color[cmyk]{1,0,1,0}, %设置注释颜色
        frame=single, %设置边框格式
        escapeinside=``, %逃逸字符(1左面的键),用于显示中文
        breaklines, %自动折行
        extendedchars=false, %解决代码跨页时,章节标题,页眉等汉字不显示的问题
        xleftmargin=2em,xrightmargin=2em, aboveskip=1em, %设置边距
        tabsize=4, %设置tab空格数
        showspaces=false %不显示空格
       }

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\setcounter{footnote}{-1} %第一个footnote不显示标号

% ----------------------------------------------------------------------------------------
% TITLE SECTION
% ----------------------------------------------------------------------------------------
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\title{
  \normalfont \normalsize 
  \textsc{\\~\\~\\~\\~\\~\\University of Science and Technology of China} \\ [25pt] % Your university, school and/or department name(s)
  \horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
  \huge 基于Inferno的集群与并行计算框架 \\ % The assignment title
  \horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\newcommand{\n}{\\\indent}
\author{\Large{阮震元~~~~解宇飞~~~~杨智~~~~刘旭彤}\footnote{阮震元~:~PB13011009,联系方式rzyrzyrzy2014@gmail.com,科技实验楼1406系统设计室}
\setcounter{footnote}{-1}
\footnote{解宇飞~:~PB13011001~~~~杨智~:~PB13011079~~~~刘旭彤~:~PB13011072}
} % Your name
\date{} % Today's date or a custom date

\usepackage{enumitem}

\lstset{
        language=text,
        keywordstyle=\color{blue}, %设置关键字颜色
        commentstyle=\color[cmyk]{1,0,1,0}, %设置注释颜色
        frame=single, %设置边框格式
        escapeinside=``, %逃逸字符(1左面的键)，用于显示中文
        breaklines, %自动折行
        extendedchars=false, %解决代码跨页时，章节标题，页眉等汉字不显示的问题
        xleftmargin=2em,xrightmargin=2em, aboveskip=1em, %设置边距
        tabsize=4, %设置tab空格数
        showspaces=false %不显示空格
       }

\newcounter{newlist} %自定义新计数器
\newenvironment{denselist}[1][可改变的列表题目]{%%%%%定义新环境
\begin{list}{\textbf{\hei #1} \arabic{newlist}：} %%标签格式
{
\usecounter{newlist}
\setlength{\labelwidth}{22pt} %标签盒子宽度
\setlength{\labelsep}{0cm} %标签与列表文本距离
\setlength{\leftmargin}{0cm} %左右边界
\setlength{\rightmargin}{0cm}
\setlength{\parsep}{0ex} %段落间距
\setlength{\itemsep}{0ex} %标签间距
\setlength{\itemindent}{44pt} %标签缩进量
\setlength{\listparindent}{22pt} %段落缩进量
}}
{\end{list}}%%%%%      


\begin{document}

\maketitle % Print the title
 
% 目录
% 可以直接编辑toc文件来更改目录
\clearpage
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\tableofcontents
\clearpage
% 改变层次目录来达到section和subsection默认不编号(需要tocvsec2包)

\section{调研报告}

\subsection{项目背景及实践意义}

\subsubsection{项目的背景}
如今人类各个学科繁多、涉及面广、分类细致，而且不同学科之间又有交叉.而如今很多的学科都需要进行大量的计算，如天文学研究组织需要计算机来分析太空脉冲，星位移动；生物学家需要计算机来模拟蛋白质的折叠过程；药物学家需要计算机来研制克服爱滋病或非典的药物；数学家需要计算机来计算最大的质数和圆周率的更精确值；经济学家也要用计算机分析计算在几万种因素考虑下某个企业/城市/国家的发展方向从而宏观调控.由此可见，未来科学的发展离不开计算.在很多时候，为了进行研究而特地去购买一台专用的超级计算机往往是不现实的.而分布式计算，因为其高效而便宜的特点，越来越受社会的关注.\n
为了说明目前国内分布式计算的现状，我们参考了BOINC平台的一些数据. \n
BOINC，也即伯克利开放式网络计算平台（Berkeley Open Infrastructure for Network Computing)，是目前主流的分布式计算平台之一.作为一个高性能的分布式计算平台，截止至2015年1月16日，BOINC在全球有235,980位活跃的参与者以及692,208台活跃的主机，提供约9.871 petaFLOPS的运算能力(天河二号超级计算机运算速度约为33.86 petaFLOPS).\n
以生命科学为例，下面是一些运行在BOINC平台下的项目：
\begin{enumerate}
\item Discovering Dengue Drugs : 针对登革热、丙型肝炎、西尼罗河病毒和黄热病病毒发现有前途的药物先导化合物
\item Genome Comparison : 染色体对比研究
\item Help Conquer Cancer : 帮助科学家征服癌症
\item Help Defeat Cancer : 帮助科学家对抗癌症
\item Human Proteome Folding : 人类蛋白质折叠研究 
\item Human Proteome Folding 2 : 人类蛋白质折叠第2阶段研究
\item Help Cure Muscular Dystrophy : 针对肌肉营养失调和其他神经肌肉型疾病的研究
\item Nutritious Rice for the World : 针对水稻预测蛋白质结构的研究，以提高水稻产量
\end{enumerate}

\subsubsection{分布式操作系统}
分布式操作系统（Distributed operating system)是在互不依赖的、联网的、相互通信的、物理上分离的多个节点上的软件.每个独立的节点包含一部分整体的操作系统上软件的子集.每个子集都可分为两个不同的部分.第一部分是一个通用的微内核,这个内核直接控制节点上的硬件资源.第二部分是一些高层次的系统管理程序,用来协调节点上独立的和相互协作的活动.这两个部分可以把多节点上的资源和处理能力整合成一个高效并且稳定的系统.因此,尽管这个系统上包含了很多的节点,但是在用户和应用程序看来,这就像是一个节点一样.

\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=1.5in]{pic/distribute_os.png}
\caption{分布式操作系统的架构}
\end{figure}


\subsubsection{分布式计算}
分布式计算是指把一个需要拥有强大计算能力的超级计算机才能解决的问题分割成小部分,通过分配给许多计算机单独进行计算,整合汇总的局部结果最终得到最终结果,达到虚拟计算机解决大型问题的效果.\n
如今很多领域都采用了这种用分布式系统搭建集群并实现并行计算的方式来进行复杂问题的处理.例如蛋白质疾病分布式计算项目.这个项目主要研究蛋白质折叠、误折、聚合及由此过程引起的一些相关疾病的分布式计算工程.该项目使用联网式的计算方式和大量的分布式计算能力来模拟蛋白质折叠的过程,该过程需要尽量大量、复杂的计算.通过利用世界各地闲散的计算资源,我们可以加快该项目的进程.

\subsubsection{分布式系统实现并行计算的主要问题}

\begin{enumerate}
\item 分布式的数据和文件管理 : 在搭建起大规模的集群时,必须考虑数据分布存储的管理以及访问问题.唯有解决了这个问题,才能够保证并行计算结果的正确性以及并行计算的性能.
\item 数据访问和通信控制 : 在分布式存储访问结构的系统中,数据在不同节点间传输,由于不同节点的运算速度不一致,此外,还有数据访问／通信的时间延迟等问题的存在,所以必须考虑数据和计算同步问题.
\item 可靠性 : 在某些情况下,如网络连接失败,节点可能会失效,进而导致数据丢失、程序终止、乃至系统崩溃等问题的发生.因此,为保证可靠性,必须考虑数据丢失后的恢复以及程序和系统崩溃后的恢复.
\item 网络互联结构的选择 : 不同的节点链接的结构会对系统整体的可靠性以及整体的运算速度产生重大影响.因此,必须根据实际需要、实际情况,权衡各个因素,选择最合适的节点链接结构,如主从非对称结构、环形结构、互联网络结构等.
\item 并行计算的任务划分和算法设计 : 并行计算需要将一个大的计算任务分解成数量合适的小任务,把这些小任务分配给不同的节点或者处理器进行处理,最终收集各个节点／处理器返回的局部结果,并最终整合为一个大的结果.其中并行计算的形式主要有算法分解和数据划分.在分解和划分时,要充分利用各节点、处理器的性能,从而提高运算速度.
\item 并行计算程序设计框架的设计以及实现 : 为了保证并行计算的正确性以及高效性,程序员需要考虑各种各样繁琐的技术细节:数据的存储管理、计算任务的划分、任务的调度执行、数据和计算的同步、结果的收集、节点实效后的恢复等.如何使这些底层细节透明化、自动实现并行计算,是一个很重要的问题.
\item 如何度量并行计算的性能 : 度量并行计算性能的指标是并行计算技术的核心.
唯有有了一个确定的指标,才能设计出合适的算法以及实现方式.
\end{enumerate}

\subsubsection{Inferno操作系统}

根据调研,我们最终决定使用开源的分布式操作系统Inferno来实现分布式计算.\n
Inferno是一个由贝尔实验室研究发明的开源的分布式操作系统,现在由Vita Nuova继续发展和维护.它基于贝尔实验室plan9的经验和该实验室对os、语言、实时编译器、网络、移植等相关方面的研究.\n
\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=3.5in]{pic/inferno.png}
\caption{Inferno的架构}
\end{figure}
该系统定义了一个名为Dis的虚拟机,Dis可在不同的机器上实现.\n
Inferno也提供了一个虚拟的、提供相同借口的操作系统,这个操作系统可以让用户在硬件上直接运行Inferno或者在其它系统中以应用程序的方式运行Inferno. \n
为了让应用程序以统一的方式进行文件操作,如打开、关闭、读写,Inferno采用了一种名为Styx的通信协议.\n
作为一个分布式操作系统,Inferno的设计基于以下三个基本原则:
\begin{itemize}
\item Resources as files: 所有的资源都以分层次的文件的形式存在于文件系统中.
\item Namespaces: 在应用程序看来,网络是一个单一的、条理清楚的namespace（命名空间）.虽然这个namespace看起来像是一个分层次的文件系统,但实际上,它表示的是分散的资源.
\item Standard communication: 通过一个名为Styx的协议来访问所有本地和远程的资源.
\end{itemize}

\subsection{小组成员相关背景}
\begin{center}
  \bf{阮震元}
\end{center}
\begin{enumerate}
\item 第二届全国RDMA编程挑战赛冠军
\item 现任科大超算曙光队主力队员,7月将代表科大出征国际超算竞赛
\item 高中时获全国信息学联赛一等奖
\item Topcoder SRM DIV 1
\item Coderforces DIV 1
\item 中国最大计算器论坛cnCalc超级版主
 \end{enumerate}
\hspace{0.75cm}
5年编程开发经验,熟悉C,C++,Java,Python,Scala,Pascal,Deiphi等语言,CUDA,OPENMP,MPI等并行技术.有JavaSE,JavaWeb开发经历,Linux运维经验.高中时获信息学联赛一等奖,Topcoder SRM DIV 1,Coderforces DIV 1,有丰富的算法竞赛经历.大二参加全国RDMA编程挑战赛,具体内容是把分布式处理框架Spark\footnote{http://spark.apache.org/}的IO部分从TCP/IP协议移植到RDMA协议,大幅度提高IO吞吐率从而达到加速效果,获冠军.现任科大超算队队员,即将出征今年7月在德国汉堡举行的国际大学生超算竞赛,在队内负责大型分子动力学模拟软件LAMMPS\footnote{http://lammps.sandia.gov/}的优化加速工作.同时在队内担任系统管理员一职,现管理两个超算系统共十余个节点.

\subsection{立项依据}

\subsubsection{项目意义}
本项目的目标是在具有强大可移植性的分布式操作系统Inferno上实现一个通用并行计算框架,使用户在不了解Inferno分布式资源细节的情况下方便地开发并行程序.\n
 目前流行的并行计算框架如:基于消息传递的“裸”并行编程模型MPI:使用Nvdia加速卡的CUDA; 基于同步数据流的MapReduce,Spark,Prlter:基于数据流水线的Dryad/DryadLINQ,SCOPE,基于存储共享的Piccolo,用于图计算的Pregel,Apache等.本项目所实现的基于分布式操作系统Inferno的分布式结构和rstyx网络协议的并行计算框架属于独创性工作.\n
本项目利用Inferno系统方便的分布式资源管理解决了分布式数据存储管理的问题,通过rstys协议解决通信控制问题.\n
相比CUDA而言,我们并不需要特殊的硬件,只需每台节点上装有Inferno.对于OpenMP而言,我们不仅可以和他一样方便使用一条预处理指令即可自动对for,while等实现并行化,还可以直接看到由我们的解释器翻译出来的并行化代码(保证可读性和可扩展性),甚至可以自己再进一步修改优化.相比MPI我们则不需要实现复杂的通信接口,一切都基于Inferno强大的网络协议Styx,Rstyx.相比MapReduce,Spark而言,我们利用Inferno天然的分布式特点,使用脚本即可实现类似于Map,Reduce的函数.

\subsubsection{理论依据}

\paragraph{资源即文件}

所有Inferno的本地和远程资源都被表示为分层文件系统内的一组动态的文件,这些文件可以表示存储设备、进程、服务、网络和网络连接.应用程序可以通过使用标准的文件操作要求来操作相关的文件进而访问每个资源.使用文件为系统中心概念的优点是:
\begin{itemize}
\item 文件系统具有跨多种操作系统的简单和易于理解的界面,文件界面一般有一套定义好的操作,例如打开,读,写.
\item 通过文件系统降低了代码量,并保持Inferno系统简单、可靠和高度轻便的特性.
\item 对众所周知的文件使用惯例命名,便于统一和理解.
\item 便于确定文件的访问权限和文件许可,可以用于确保多级别的安全性.
\end{itemize}
\hspace{0.75cm}文件名和这些文件的动态的内容可以基于每个需求和每个客户端的基础上产生.例如对于一个传感器资源的数据文件在不同的时间读取就会返回不同的输出,或者每次读取都会在新的一行添加一个数据.这种特性是实现并行计算数据共享的基础之一.

\paragraph{命名空间}
 Inferno的第二个关键原理就是可计算的命名空间,通过命名空间每个应用程序会对它需要访问的资源建立一个独一无二的私有视角,所有的资源被表示为文件的层次结构,并且通过标准文件访问操作来访问,一个进程所使用的各种文件和服务都被一个单根分层文件系统组合起来,叫做命名空间.一个名称空间内访问的资源可以位于单个客户端或在整个网络中的多个服务器.\n
    命名空间系统的主要优点是应用程序可以使用的资源完全透明,在应用程序可见的命名空间中挂载的所有动态资源文件,无论是本地的还是远程的,该应用程序都可以方便的访问.例如,一个图形化的调试器通过读取在/prog目录下的动态文件来访问关于目前系统进程的信息,如果用户希望在另一台远程机器上调试,只需将原来那台机器的/prog目录挂入目前这台机器,对于调试器而言,它只是读取在/prog目录中的文件,但并不知道他们来自哪里.

\paragraph{使用标准通信协议来访问资源}
  Inferno使用一个被Inferno内核或应用程序执行的协议来表示或访问资源.因为所有的资源都被表示为文件,包括网络和网络连接,所以需要一个协议来和提供本地与远程资源的通信.这个协议是一个叫9P的文件服务协议（Styx是一个更早的变体）.这种方法就为我们利用已知的技术建立添加远程文件系统的分布式系统提供了一个自然的途径.拥有一个标准通信协议还提供了一个关注安全的点,Inferno提供了下面几种安全通信机制:
  \begin{itemize}
  \item 基于证书的用户认证.
  \item 消息加密.
  \end{itemize}

\subsection{目前所调研到的相关工作}

\subsubsection{Inferno相关调研}
对于Inferno系统的调研的目的是利用Inferno系统方便的分布式资源管理以及sytx通信协议来处理集群搭建以及并行框架问题.从目前的网络资源来看,Inferno研发的相关工作主要集中于Inferno的开发团队.\n
我们小组对于Inferno的理解主要来源于Inferno系统自带的documentation(doc目录)以及Inferno官方documentation和tutorial\footnote{http://www.vitanuova.com/Inferno/docs.html}.Bell实验室公开的Documentation\footnote{http://doc.cat-v.org/Inferno/4th\_edition}.\n
Inferno提供了两种模式native和hosted.其中hosted模式是一种应用层虚拟化,可以让Inferno运行在Linux,Win,OSX等系统中.\n
经过我们的调研,Inferno系统的内核有80余万行代码,并且无法取得到其开发日志.另外,Inferno系统已于2010年停止维护.所以我们决定只以Inferno作为分布式资源管理的工具,通过Inferno的shell脚本语言作为开发语言来构建集群并处理并行框架问题.具体的Inferno shell的用法我们参照Roger Peppé的《The Inferno Shell》\footnote{\text{www.vitanuova.com/Inferno/papers/sh.html}}一文进行运用.值得一提的是Inferno的shell非常强大,它核心功能精悍且可以动态加载第三方模块,这一点和python非常相似.\n
关于Inferno的sytx通信协议方面,我们调研到Rob Pike的关于《The Styx Architecture for Distributed Systems》\footnote{www.vitanuova.com/Inferno/papers/styx.html}的工作,了解infeno通过“资源即文件”的思想,将存储设备、进程、服务、网络通信等表示为文件,并通过文件的读、写、执行来控制各种资源或者进行交互.而sytx则是作为简单而统一的网络通信协议,构成了在系统内通信、操作资源架构的核心. 

\subsubsection{常见并行框架概况}
由于我们小组的主要任务还是对并行框架的构建,所以对于各个平台、模型、系统下的分布式计算和并行框架的调研是必不可少的.通过对文献及网络资源的查找,我们小组发现还没有在Inferno系统上构造并行框架的相关研究,所以创新性抑或是独创性是有所保证的.\n
接下来我们小组对几个比较常用的并行框架进行了一定的调研:
\begin{enumerate}
\item MPI : Message Passing Interface (MPI)是一个语言独立的对并行计算机进行编程的通信协议或通信系统.MPI支持点对点或者集群通信,具有标准化和可移植性的特点.''MPI is a message-passing application programmer interface, together with protocol and semantic specifications for how its features must behave in any implementation''\footnote{Gropp, William; Lusk, Ewing; Skjellum, Anthony (1996). ''A High-Performance, Portable Implementation of the MPI Message Passing Interface''. Parallel Computing. CiteSeerX: 10.1.1.102.9485}.在传输层MPI利用了sockets和TCP.MPI模型也因其高性能、可移植性、可扩展性而广泛应用于高性能计算中.
\item OpenMP : Open Multi-Processing是一种支持多平台进行shared memory multiprocessing programming的应用程序接口.OpenMP的核心元素包括进程创建、负载分配、数据环境管理、进程同步等.除了其优秀的跨平台能力外,在集群中OpenMP还可与MPI模型进行混合处理问题.

\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=2in]{pic/openmp.png}
\caption{OpenMP原理}
\end{figure}

\item CUDA : Compute Unified Device Architecture(CUDA)是由英伟达公司创造的,用于GPU上的平行处理平台.利用CUDA,GPU不仅可以进行图形处理,还可进行其他的通用计算（所谓的GPGPU）.

对于操作系统,它支持windows、linux、mac os三大主流系统.CUDA为开发者提供了虚拟指令集和并行计算单元的内存来进行运算.

\item TBB（Intel Threading Building Blocks）：是一个由英特尔公司开发的C++模板库，主要目的是使得软件开发者更好的利用多核处理器.该库为开发者提供了一些线程安全的容器和算法，使得开发者无需过多关注系统线程的创建、同步、销毁等操作，将精力集中于业务逻辑的并行化，可以与OpenMP互为补充.
\end{enumerate}

\begin{table}[!hpb]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    比较项目 & OpenMP & TBB & MPI \\ 
    \hline
    并行粒度 & 线程 & 线程 & 进程 \\
    内存模式 & 共享内存&共享内存&分布式内存\\
    适用环境 & 单机 & 单机 & 集群\\
    通讯机制&*&*&消息传递 \\
    易用性 & 高& 中 & 低 \\
    跨平台性 & 是&是&是\\
    并发数据结构 & 不支持 &支持&支持\\
    可扩展内存分配&不支持&支持&不支持\\
    \hline
  \end{tabular}
\caption{各种并行计算框架对比}
\end{table}

\subsection{OpenCL调研}

\subsubsection{简介}

如何提高计算机的性能？目前在这方面主要有两种方式：第一种方式是增加处理器的核心数来支持多线程、多任务，从整体上提高计算机的性能。第二种方案是通过异构计算框架，利用CPU，GPU乃至APU等计算设备的计算能力来提高计算机性能。\n

如今异构系统越来越普遍，但各厂商一般只提供对自己设备编程的实现。而在异构系统上用同种风格的编程语言来实现异构编程并将不同设备作为统一的计算单元来处理的难度是很大的。\n 
于是，OpenCL（Open Computing Language，即开放式计算语言）应时而生。OpenCL定义了一套机制来实现硬件独立的软件开发环境，将不同类的硬件结合到同种执行环境中。在用户看来硬件层是透明的。利用OpenCL可以充分利用设备的并行特性，支持不同级别的并行，并且能有效映射到由CPU，GPU， FPGA和将来出现的设备所组成的同构或异构、单设备或多设备的系统中。\n

OpenCL(Open Computing Language, 开放计算语言)是一个由非盈利性技术联盟Khronos Group进行管理的异构编程框架。OpenCL框架用于开发可以在各种设备上运行的应用程序。OpenCL支持多种层次的并行，可以高效映射到同构或异构的体系结构上，比如单个或多个CPU、GPU和其他类型的设备等。OpenCL提供了一个系统中设备端语言和主机端控制层两方面的定义。设备端语言可以高效映射到众多内存系统架构上。主机端语言的目标是以较低开销来高效管理复杂的并发程序。两者共同为开发人员提供了一种从算法设计高效过渡到实现的途径。

\subsubsection{主机设备交互}

OpenCL平台模型定义主机和设备的角色，并为设备提供了一个抽象的硬件模型。 \n
在平台模型中，一个主机协调在一个或多个OpenCL设备上的程序执行。平台可以被看作是厂商特定的OpenCL API实现。因此，平台上的设备只限于厂商知晓如何进行交互的设备。例如，如果选择A公司的平台，他就无法与B公司的GPU进行通信。 \n
平台模型还带来了一个抽象的设备架构，编程人员在编写OpenCL代码时以它为目标。厂商将这个抽象架构映射到具体的硬件。考虑到可扩展性，平台模型将一个设备定义为一系列的计算单元(compute unit)，每个计算单元功能独立。计算单元又进一步划分为处理部件(processing element).

\subsubsection{OpenCL架构}

OpenCL由两部分组成：第一部分是用来编写内核程序的语言，第二部分是用来定义并控制平台的API。\n
为了解决一个实际问题，我们可以用“分治”策略将问题分解为四个模型：

\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=2.3in]{pic/opencl-host-device.png}
\caption{OpenCL的平台模型--host~\&~device模型}
\end{figure}

\paragraph{平台模型} 该模型描述内部单元之间的关系。主机(host)可以是个人计算机或超级计算机。OpenCL设备(device)可以是CPU 、 GPU 、DSP或其它处理器。每个OpenCL设备包含若干计算单元(compute unit ,CU),每个计算单元又由若干处理单元(processing element ,PE)组成。 \n
OpenCL通过平台实现主机与设备间的交互操作。主机管理着整个平台上的所有计算资源,所有OpenCL应用程序都是从主机端启动并在主机端结束的。应用程序运行时由主机提交命令(command),在设备上的处理单元中执行计算。每个计算单元内所有的处理单元都会执行相同的一套指令流程。每个处理单元以单指令多数据SIMD或单程序多数据SPMD模式运行指令流。 \n
然而，一般而言，选定了一个平台，那么就只能在这个平台所支持的设备上作计算了。用了Intel SDK就只能够用Intel的CPU进行计算，用了APP SDK就只能在AMD的CPU和GPU上进行计算。而且，一般两个不同的平台间时不能进行通信。

\paragraph{执行模型}OpenCL执行两类程序:内核程序(kernel)和主机程序(host program);前者由若干个OpenCL设备执行,后者由主机执行。OpenCL通过主机程序定义上下文(context)并创建一个被称为命令队列(command-queue)的数据结构来管理内核程序的执行。在命令队列中,内核程序可顺序执行(In-order Execution)也可乱序执行(Out-of-order Execution)。 \n
每当主机提交内核程序到设备上执行时,系统便会创建一个Ｎ维(N可取1,2,3)的索引空间NDRang。内核程序将索引空间中的每一点用一个工作项(work-item)来表示，将若干个工作项划分成一个工作组(work-group)。在一个计算单元内可运行同一个工作组中的工作项，并且该组内的工作项可以并发执行在多个处理单元上。 \n
执行模型可以进而再划分为内核，上下文，命令队列．

\begin{enumerate}
\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=3in]{pic/opencl-kernel.png}
\caption{OpenCL's kernel}
\end{figure}
\item 内核 : 内核是执行模型的核心，它执行在设备上。在执行内核之前，需要先指定NDRange（N维度的范围）。NDRange是一个一到三维的index。这个index里节点的总f数目记为全局工作节点数。把全局工作节点分组，我们可以方便地管理各个节点。\n
全局工作节点数＝工作组数 $\times$ 工作组中节点的数目。如图1.5，一共包含144个节点，分为9个工作组，每个工作组包含16个节点。 \n 
通过指定合理的节点数、工作组数，我们可以提高程序的并行度。
\item 上下文 : 一个主机要使得内核运行在设备上，必须要有一个上下文来与设备进行交互。 上下文是一个抽象的容器，用来管理在设备上的内存对象并且跟踪在设备上创建的程序和内核。主机通过上下文来与设备交互，使得内核运行在设备上。
\item 命令队列 : 每个设备都有一个自己的命令队列，而主机则负责把命令发送到对应设备的命令队列上。命令队列对在设备上执行的命令进行调度。在主机和设备上，命令是异步执行的。
\end{enumerate}

\paragraph{内存模型}
\begin{enumerate}
\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=4.8in]{pic/memory.png}
\caption{OpenCL的内存模型}
\end{figure}
\end{enumerate}
OpenCL把内存模型抽象化了，通过这样来方便用户编写程序。在图1.6中，内存的分类如下：
\begin{itemize}
\item 全局内存 ：所有节点都可以对其进行读写。
\item 常量内存 ：全局内存中的一块区域，在内核的执行过程中保持不变。 由主机来负责分配和初始化其中的内存对象。
\item 局部内存 ：由一个工作组里各个工作节点共享的内存区域。在OpenCL设备上，它可能会作为一块专有的内存区域存在，也可能会被映射到全局内存中。
\item 私有内存：仅属于一个工作节点的内存区域。
\end{itemize}

\paragraph{编程模型}
数据并行和任务并行是OpenCL可以支持的两种并行编程模型,同时两者的混合模型也得到支持。通常情况下,OpenCL采用的首要模型是数据并行,而对多核CPU主要采用任务并行。\n
数据并行编程模型中,一系列的指令会作用到内存对象(memory obect)的多个元素上。严格来说,数据并行要求内存对象单元与工作项保持一对一的映射,而在实际应用中,并不要求严格按照这种方式。在数据并行编程模型中,OpenCL又提供了一种分级方式,有两种方法:显式分级模型和隐式分级模型;前者要求开发人员指出工作项的总数和工作项所属的工作组;而后者仅需要开发人员定义工作项的总数,对于工作项的划分则根据OpenCL的实现来管理。在任务并行编程模型上,每个工作项都相当于在一个单一的计算单元内,该单元内只有单一工作组,该工作组只有该工作项本身在执行。

\paragraph{OpenCL软件框架}OpenCL软件框架包含三部分:OpenCL平台层、OpenCL运行时和OpenCL编译器。在OpenCL平台层上,开发人员可以查询系统中的平台数目并选定运行平台,在指定的平台上选择必要的计算设备并对它们进行初始化,然后可以建立上下文, 并创建命令队列。执行内核程序、读、写及复制缓冲区和同步操作等都是通过命令队列中的命令实现的。一个命令队列和一个OpenCL设备是一对一的关系。在OpenCL运行时中,开发人员建立内核实例,并将其映射到正确的内存空间中,接着在命令队列中排队执行内核。OpenCL编译器负责编译运行在设备上的程序,并创建可执行程序。

\paragraph{OpenCL实现原理}

从编程的角度来说, OpenCL中的操作都是和一个给定的上下文环境有关, 每个上下文环境中都有若干个相关的设备。在上下文环境中, OpenCL能够保证设备之间的内存一致性。OpenCL使用buffers(1维的内存块)和images(2维和3维的内存块)来存储内核数据。一旦分配了存放内核数据的内存空间, 并且指定了运行内核程序的设备,就需要装载和编译内核程序。为了执行内核程序, 程序员必须建立一个内核对象(kernel object), 并且设置内核参数。之后,内核就可以在命令队列中排队执行了。通常情况下, OpenCL设计的大致流程为:
\begin{itemize}
\item 创建并初始化OpenCL设备和上下文环境,建立命令队列;
\item 创建并编译源程序,建立内核句柄;
\item 分配数据所需内存空间,并将数据复制到OpenCL设备上;
\item 设置内核参数; 
\item 执行内核程序;
\item 将计算结果从OpenCL设备复制到主机中;
\item 释放系统所占资源。
\end{itemize}

\subsection{MPI调研}

MPI 是一种消息传递编程模型并可以被广泛使用的编写消息传递程序的标准。它由一组库函数组成,在Fortran或C 的基础上扩展为一种并行程序设计语言。严格来说,MPI只是一个库,提供了应用程序的编程接口,方便Fortran77/90或C/C ++等编程语言直接对这些库例程或函数进行调用,实现进程间通信。

\subsubsection{MPI数据类型}
为了支持异构环境和易于编程,MPI定义了精确的数据类型参数而不使用字节计数,以数据类型为单位指定消息的长度,这样可以独立于具体的实现,并且更接近于用户的观点。对于C和Fortran,MPI均预定义了一组数据类型(如MPI\_INT 、MPI\_DOUBLE等)和一些附加的数据类型(如MPI\_LONG\_LONG\_INT等)，这些与语言中的数据类型相对应。MPI 除了可以发送或接收连续的数据之外,还可以处理不连续的数据,如多维向量或类C 语言中的结构数据类型。为了发送不连续的数据,MPI定义了打包(pack)与解包(un-pack)操作,在发送前显式地把数据包装到一个连续地缓冲区,在接收之后再从连续缓冲区中解包。MPI 还允许发送和接收不同的数据类型,使得数据重映射方便灵活。通过使用不同的数据类型调用MPI\_SEND ,可以发送不同类型的数据,使得MPI 对数据的处理更为灵活。

\subsubsection{通信域}
通信域是MPI的一个关键概念,它以对象形式存在,作为通信操作的附加参数。通信域为开发消息传递程序提供了模块化支持,从而强有力地支持开发并行库和大规模代码。MPICH 中的一个通信域定义了一组进程和一个通信的上下文,虚拟处理器拓扑、属性等内容。通信上下文是通信域所具有的一个特性,它允许对通信空间进行划分,提供了一个相对独立的通信区域,由系统严格管理,对用户是透明的,有力地保证了库代码的消息通信互不干扰。MPI_COMM_WORLD是一个由MPI提供的预定义的通信域,它包括所有的进程。所有的MPI实现都要求提供MPI_COMM_WORLD通信域,在进程的生命期中不允许将其释放。用户也可以在原有通信域的基础上,定义新的通信域。通信域为库和通信模式提供了一种重要的封装机制。

\subsubsection{MPI调用接口}
MPI-1提供了128个调用接口,MPI-2提供了287个调用接口,但MPI所有的通信功能可以用它的6个基本调用来实现,也就是说可以用这6 个基本调用来完成基本的消息传递并行编程。
\begin{enumerate}
\item MPI初始化:MPI\_INIT()是MPI程序的第一个调用,它完成MPI程序所有的初始化工作,也是MPI程序的第一条可执行语句。
\item MPI 结束:MPI\_FINALIZE()是MPI程序的最后一个调用,它结束MPI程序的运行,也是所有MPI程序的最后一条可执行语句。
\item 获取当前进程标识:MPI\_COMM\_RANK(comm ,rank)返回调用进程在给定的通信域中的进程标识号,有了这一标识号,不同的进程就可以将自身和其他的进程区别开来,实现各进程的并行和协作。指定的通信域内每个进程分配一个独立的进程标识序号,例如有n个进程,则其标识为0\~n-1。
\item 通信域包含的进程数:MPI\_COMM\_SIZE(comm ,进程通过这一调用得知在给定的通信域中一共有多少个进程在并行执行。size)返回给定通信域中所包括的进程的个数,不同的进程通过这一调用得知在给定的通信域中一共有多少个进程在并行执行。
\item 消息发送:MPI\_SEND(buf ,count ,datatype ,dest ,tag ,comm):MPI\_SEND将发送缓冲区buf中的count 个datatype数据类型的数据发送到目的进程dest,tag是个整型数,它表明本次发送的消息标志,使用这一标志,就可以把本次发送的消息和本进程向同一目的进程发送的其他消息区别开来。MPI\_SEND操作指定的发送缓冲区是由count个类型为datatype 的连续数据空间组成,起始地址为buf。
\item 消息接收:MPI\_RECV (buf, count, datatype, source,tag, comm.status)从指定的进程source接收消息,并且该消息的数据类型和消息标识和本接收进程指定的datatype和tag相一致,接收到的消息所包含的数据元素的个数最多不能超过count 。status是一个返回状态变量,它保存了发送数据进程的标识、接收消息的大小数量、标志、接收操作返回的错误代码等信息。接收到的消息的源地址、标志以及数量都可以从变量status中获取,如在C语言中,通过对status.MPI\_SOURCE, status.MPI\_TAG, status.MPI\_ERROR引用,可以得到返回状态中所包含的发送数据进程的标识,发送数据使用的tag标识和本接收操作返回的错误代码。
\end{enumerate}

\subsubsection{MPI并行编程模式}
MPI 具有两种最基本的并行程序设计模式:对等模式和主从模式,大部分并行程序都是这两种模式之一或二者的组合。并行程序设计的两种基本模式可概括出程序各个部分的关系。对等模式中程序的各个部分地位相同,功能和代码基本一致,只是处理的数据或对象不同。而主从模式体现出程序通信进程之间的一种主从或依赖关系。并行算法是并行计算的核心,用MPI 实现并行算法,这两种基本模式基本上可以表达用户的要求,对于复杂的并行算法,在MPI中都可以转换成这两种基本模式的组合或嵌套。

\paragraph{主从模式}
主从模式又称为 Master/Slave 模式,在这种模式的并行程序中,存在一个单独执行控制程序的主进程(Master),负责任务的划分、分派、结果的收集,负责所有进程间的协调和网络调度,并且在其它结点机调用Slave 程序等.执行从程序的若干进程称为从进程(Slave),负责子任务的接收、计算和结果的发送.\n
在这种模式下,一个程序由两部分构成:Master 程序和Slave 程序.主进程执行Master 程序,各子进程分别执行各自的Slave程序.主进程通过消息传递通信函数完成与并行处理机节点上运行的各子进程间的数据传输.Master 程序和Slave程序各自的程序框架如下:
\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=5in]{pic/MPI-master-slave.png}
\caption{MPI主从模式}
\end{figure}

\paragraph{对等模式}
对等模式又称为 SPMD 模式,在这种模式的并行程序中,各个部分地位相同,所有进程执行同一个程序,只是各进程计算的数据不同,控制和处理都在计算节点上. 各节点使用消息实现同步或异步通讯.对等模式程序基本框架如下:
\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=5in]{pic/MPI-equal-mode.png}
\caption{MPI对等模式}
\end{figure}


\subsection{想法来源及可能遇到的挑战}
并行计算如果按照硬件环境进行分类,那么以分布式计算为基础的计算机集群使其中典型的一类.以上三种并行框架大多是跨平台的,而inferno自身却具有分布式系统和以“资源皆文件”的优势,使得并行计算框架的构建更具有可执行性,但这个方向还鲜有相关研究.\n

根据相关调研,我们可能会面对下述问题 : 
\begin{enumerate}
\item 计算机集群实现分布式运算的特点,计算过程不一定是同步的,所以负载均衡是一个有挑战的问题,对应解决方案有Round-robin DNS,scheduling algorithms等.
\item 由于分布式计算需要共享实时数据,对于网络带宽以及低延迟网络的需求也是其需要考虑的问题,可以运用更高效率的传输媒介来处理.
\item 因为考虑的是并行框架,各个计算机、各个数据以及各个指令之间的竞争需要处理.例如:当计算机尝试覆盖相同或者旧的数据,而此时就的数据仍在被读取.结果可能是下面的一个或者多个情况:机器宕机、出现非法操作并结束程序、错误的读取旧数据、或者错误的写入新数据.在这里,竞争的危害出现于通过访问同一个信息通道的数据.解决的方法可以通过标记正在处理的数据以及标记读写来处理.   
\item 分布式系统中的另一个问题是容错性问题（fault-tolerance）.这也是有名的拜占庭将军问题（Byzantine failures）.在容错的分布式计算中,拜占庭失效可以是分布式系统中算法执行过程中的任意一个错误.这些错误被统称为“崩溃失效”和“发送与遗漏是失效”.当拜占庭失效发生时,系统可能会做出任何不可预料的反应.这些任意的失效可以粗略地分成以下几类;进行算法的另一步时失效,即崩溃失效:无法正确执行算法的一个步骤;执行了任意一个非算法指定的步骤.具体来说就是所写的并行框架在没有容错处理时出现让人匪夷所思的随机错误.具体可以通过口头消息算法或签名消息算法解决.
\end{enumerate}
 

\section{可行性研究报告}

\subsection{理论依据与技术依据}

\subsubsection{Sytx协议及其具体实现}

Inferno的文件系统协议（或Sytx）是贝尔实验室九号项目所开发的针对于分布式操作系统的网络协议，作用是链接系统内的组件.Inferno系统中文件是核心，这些文件代表了视窗、网络连接、进程、以及其他存在于操作系统中的各种元素.不同于NFS，Styx的用途是把数据缓存起来，并提供虚拟文件的机制（例如/proc用以表示进程）.\n
在Inferno系统中，Inferno服务器（Inferno server）是被用来被Inferno进程访问的并提供分等级文件系统（文件树）的代理.一个服务器响应客户端的请求并导引文件树中的具体位置，同事可以对文件进行创建、删除、读写等操作.\n
对于某个服务器来说，连接是通过一个客户端和服务器间的双向的通信通道来实现的.客户端可能是单一的，也可能有多个客户共享同一连接.一个服务器文件的文件树通过bind和mount命令联结到一个进程组的命名空间上.这样，组内进程就是服务器的客户端：对于文件的系统调用就被翻译为请求和响应，并得到相应的服务.\n 
      Inferno的文件协议，Sytx，就是用于处理客户端和服务器间的消息（message）的.目前我们组所用的Sytx版本是和9P2000一致的.客户端给服务器传递请求（T-messages）.相应的，服务器返回响应给客户端（R-messages）.这样就构成了基本的发送和接收机制.关于message的样式，这里就不再赘述.不过值得一提的是fid——一个客户端用来辨认服务器上当前文件的32位无符号整型.\n 
      Styx在客户端及服务器端提交如下的消息.这些消息对应到虚拟文件系统层的进入点，所有的服务器都必须实现这些消息：
\begin{enumerate}
\item version : 交涉协议的版本.
\item error : error.
\item flush : 终止消息.
\item auth, attach : 打开连接.
\item walk : 走访目录层次结构.
\item create, open : 准备一个用来写入/读取既有或新增文件的fid.
\item read, write : 发送数据给文件或从文件接收数据.
\item clunk : 抛弃fid.
\item remove : 从服务器移除文件.
\item stat, wstat : 查询或变更文件属性.
\end{enumerate}
这些具体实现中，有几个是我们工程的技术依据：
\begin{enumerate}
\item walk message : walk消息使得服务器变更当前的与fid相联系的文件为一个在目录中旧的“当前文件”或者某一个它的子目录.Walk返回一个新的fid，这个fid指向于该文件.通常，客户为根目录保留fid，然后通过walk从根fid进行导引.
\item attach，auth message : attach消息用来从客户端的用户向服务器提供一个连接点（introduction）.这条消息识别用户并选择文件树去访问.attach消息会使客户端会拥有一个指向目标文件树根目录的连接，当然这个目录通过fid来被识别.Auth消息包含一个afid，用来进行认证.一旦认证协议完成，相同的afid会被attach提供给用户来保证成功进入并获取服务.
\item stat，wstat ：stat消息获取文件信息.Stat字段包括文件名，读写、执行权限，访问和修改次数，所有者和组的信息.Wstat允许默写文件的属性被做某些改动.
\item flush : flush信息可以终止某个请求.当服务器收到 Tflush，它就不会回复带有oldtag标记的消息并立刻发送Rflush.客户端必须等待直到得到Rflush.这时oldtag会被回收.
\item write，read，open，create，remove功能都如上面简介介绍.
\end{enumerate}
~\n  另外，Styx对于安全性也做了一定工作，提供了许多安全机制来处理会影响系统整体性和安全性的危险动作.统一的文件通信协议包含着用户和组的识别码.比如，在收到一个打开文件请求时，服务器会检查并匹配用户id.这个机制和通用操作系统比较类似.Styx用通过网路连接的文件系统方式提供远程资源.这种获取远程资源的方式对应用程序是透明的，所以认证不需要特别提供.例如，在inferno中，客户端和服务器间的通信渠道中会包含许多加密和消息摘要的协议.与通用文件服务器和一些电话通讯领域一样，所有对于资源的运用都会经过一定的认证，这也保证了远程管理的安全性.\n
  值得一提的是Styx协议将请求翻译为必要的字序列并将其在通信通道中传递.所以Styx协议适应于ISO标准的OSI Session Layer Level.所以它独立于机器结构并成功地应用于不同指令集和数据格式的机器中.

\subsubsection{ns - 显示当前命名空间}
 在立项依据中，我们简要介绍了命名空间的概念，在Inferno系统中，命名空间是一系列挂载点（mount points）和绑定（binds）的集合，与UNIX系统的挂载点相似.在Inferno系统中，你可以把一个使用styx协议的服务器挂载在一个文件描述符上（如一个网络连接，或者一个程序的管道)，因此，挂载在命名空间中引入了一个新的文件树；另一方面，挂载还可以仅仅使命名空间的一部分作为一个别名出现在另一个命名空间中.\n
命名空间包括两个部分：常规路径，以“/”开始；和一个“特殊”的路径集合，以“\#”开始，后面跟单个字符.这些特殊路径是内核设备，为内核服务的文件树，频繁访问的硬件（如硬盘）或者内核数据结构（如处理器）.只有命名空间中的常规路径可以通过bind、mount、unmount指令修改.ns命令用来打印当前的命名空间，在shell中执行ns命令打印出的是shell的命名空间.为了举例说明，下面是在初始Inferno下运行ns的输出：

\lstinputlisting{result/ns} ~\n
 如上面的输出所示，大多数行在常规文件系统中bind一个内核设备，例如\#U（Inferno根目录的内容）在/，\#m（鼠标）在/dev上，\#I（网络栈）在/net上.-a和-b选项分别表示第一条路径的内容出现在原来第二路径的后面和前面（第二条将会包含第一条路径和它自己的集合）.如果出现选项-c，目标路径将允许文件创建.上面出现的命名空间是被Inferno初始化代码安装的，作为一种引导系统的方式给出一个合理的默认命名空间.
\n
挂载在命名空间中引入了一个外部文件树，就像UNIX系统mount一样，文件树被文件描述符上使用styx协议访问.内核通过翻译open/read/write/stat/stc系统调用成styx信息来控制styx部分，然后在返回值中回复信息.mount系统调用要求文件描述符使用styx协议通信.mount程序挂载3种类型的styx服务器拥有很方便的语法：

\begin{itemize}
\item mount /path/to/styx/file target, 通过打开/path/to/styx/file取得文件描述符
\item mount net!www.example.org!styx target, 通过访问net!www.a.org!styx取得文件描述符
\item mount {program} target, 开始程序并且用它的标准输入作为文件描述符
\end{itemize}

ns显示给定pid或者默认它本身的命名空间结构，以/prog/pid/ns的内容为基础，打印一系列bind和mount命令，如果它被执行，会重建一个相同的命名空间.如果任何涉及到的文件因mount或bind命令被改名，那么就显示文件的原始名字.

\subsubsection{bind, mount, unmount - 修改命名空间}

  bind和mount命令修改当前进程和在一个命名空间组中其他进程的命名空间，目标是一个存在的文件或当前将要修改的命名空间的目录名.对于bind，目标是一个已经存在的文件或当前命名空间下的目录名.在bind命令成功执行之后，目标文件名就是原来的东西起的一个别名；如果修改没有隐藏它，目标将依然指向他的原来的文件.源和目标必须是同一个类型，要么都是目录，要么都是文件.对于mount，源可以是一个shell命令，一个网址，或者一个文件名，如果源被花括号扩起来，那么它将作为一个sh命令的调用，如果源中包含感叹号或者没有文件，他将被作为一个网址.
\n
 bind和mount命令的工作可以被unmount命令撤消，如果给unmount两个参数，他会撤消相同参数的bind或mount命令，如果只给它一个参数目标上所有bind过和mount过的东西都会被撤消.注意，当使用bind或unmount命令时，内核设备名称中的\#字符必须被引号引起来否则shell会把它看做注释的开头. \n
修改命名空间的常用选项：
\begin{itemize}
\item  -b : mount和bind均有效.把源目录加入被目标目录代表的联合目录开头.
\item  -a : mount和bind均有效.把源目录加入被目标目录代表的联合目录结尾.
\item  -c : 这个选项可以加在上面两个选项上，用来允许在联合目录中的创建.当在联合目录中创建一个新的文件时，它将被放在联合目录中第一个mount或bind带有-c选项的元素中，如果那个目录没有写权限，将会创建失败.
\item  -q : 如果bind或mount失败，不打印诊断信息，安静退出.
\item  -A : 只对mount有效.在执行mount之前不认证服务连接.
\end{itemize}

\subsubsection{os - 寄主操作系统接口（对寄生安装的Inferno有效）}
os指令将使得我们使用C语言开发的程序在Inferno系统中运行.\n
  os使用使用cmd设备在寄主系统执行命令，如果存在-m选项，os会使用在挂载点的设备，否则将会被假设在/cmd下，如果必要还会被bind在本地命名空间下.-d选项将会导致命令运行在dir目录下，如果dir目录不存在或者无法访问就会报错，且指令不会执行.命令的标准输出和标准错误会出现在os命令本身的标准输出和标准错误上，os复制标准输入到远程命令的标准输入；如果命令没输入，就会重定向os的输入到/dev/null.当cmd终止后，os命令就终止了，它的退出状态返回的是cmd的退出状态.如果命令被杀死或退出（比如缺少输入或输出），寄主自己的进程控制办法将试图杀死仍在运行的cmd，-b（background）选项将制止这个行为.-n选项导致cmd在低于正常优先级下运行.-N选项把低优先级设置为特殊的等级，从1到3.

\subsubsection{cpu - 执行一个远程的命令}
cpu命令向主机拨号（使用tcp网络如果网络没有显式给出）.连接后，向外传输本地的命名空间并执行远程机器上的指令.本地命名空间对于/n/client中的命令是可见的；本地的设备文件被bind到了远程设备目录下.如果命令没有给出，那么/dis/sh就处在运行之中. \n
-C选项设定了authentication被摘要或者加密的算法，具体的摘要算法有：MD4,MD5等，具体的加密算法有：RC4,DES,CBC,ECB等算法.默认的是不用任何算法.

\subsubsection{file2chan - 节点间通讯}
因为我们要实现并行计算框架,一个亟待解决的问题就是如何做节点间通讯.最初我们的想法是直接用一个文件来做共享池.然而后来在此基础上进行实现时发现困难重重:
\begin{enumerate}
\item 直接用文件做共享难以解决同步的问题.因为直接用文件做共享池,这样在运行时没法动态的加锁,解锁,不方便解决同步问题.
\item 难以实现主节点的动态任务分配.
\item 难以实现主节点对从节点请求的动态响应.
\item 光是一个文件难以做到传输各种格式不同的通讯信息.
\item 直接在磁盘中暴露中间计算内容,缺少加密.
\end{enumerate}
~\n
经过我们重重调研,发现Inferno提供了一个叫做file2chan的脚本工具.\n
file2chan是一个sh的可加载模块，可以在命名空间中创建一个可以由shell脚本决定其性质的文件.file2chan在命名空间中创建一个文件名，同时产生一个新的线程来为这个文件服务，如果成功了，环境变量\$apid就会被设置成新线程的pid，否则返回一个错误状态（non-nil）.readcmd、writecmd和closecmd都应该是可执行的指令块.之后，每当一个进程从文件名读入，就会调用readcmd；每当一个进程对文件名写入，就会调用writecmd；每当一个从文件名打开的文件关闭，就会调用closecmd.\n
如果我们采用file2chan,则可以轻松的解决上述问题.具体设想与实现可以参考概要设计报告部分.

\subsection{创新点}

\subsubsection{良好的异构支持}
由于Inferno的分布式特性.我们的并行框架具有天然的跨平台性，可以实现arm, x86, amd64节点混合成的混合集群．\n
同时我们借鉴了OpenCL的平台模型，并采用抢占式任务分发以及细并行粒度的任务分配，可以使得计算能力相差很大的节点协作计算．具体内容可以参见概要设计报告中异构的部分．

\subsubsection{实现MapReduce计算模型}
尚未有人、或极少人在Inferno上做过并行计算.然而，Inferno系统本身的特性使得它能很方便的实现并行计算.为了进一步完善我们小组的并行计算模型，我们也在Inferno上实现了MapReduce这一编程模型.通过MapReduce, 我们可以把一个大作业拆分成多个小作业，而用户只需要决定拆分成多少份，以及定义作业本身.因此，不熟悉并行编程的程序员也能充分发挥分布式系统的威力.事实证明，Inferno在实现并行计算框架这一点上较其它系统有天然的优势，只要进行少量的工作就完成了MapReduce在Inferno上的移植.

\subsubsection{多节点视为整体的一个节点}
集群是一组相互独立的、通过高速网络互联的计算机，它们构成了一个组，并以单一系统的模式加以管理.如今主流的集群实现方式中，集群中节点间的数据交换通过数据共享空间或者点对点的通信实现，各个节点事实上并不能真正成为一个整体，例如基于MPI(Message Passing Interface)的并行框架.而inferno通过导入Namespace（命名空间），使所有的资源都以分层次的文件的形式存在于文件系统中，因此应用程序可以以同样的方式访问远程的节点和本地节点.故而虽然这个系统上包含了很多的节点, 但是在用户和应用程序看来, 这就像是一个节点一样. 

\subsubsection{Inferno在64位平台上的移植}
最新的inferno第四版发行于2004年，支持的平台有ARM,PA-RISC,MIPS,PowerOC,SPARC,X86.但如今市面上的平台基本上都是64位的了，因此要想在64位的平台上运行inferno，就要进行移植.为了让Inferno能成功地在市面上绝大多数电脑上运行，把更多的平台纳入我们的并行框架中，我们修改了Inferno的部分代码，将其移植到了64位的平台上.

\subsubsection{以外部解释器的形式实现并行计算}
之前调研了其他著名并行计算框架的使用方式:
\begin{itemize}
\item CUDA, MPI : 用户根据具体的MPI实现框架(Intel MPI, OpenMPI等)给定的API接口,调用具体的库函数,进行串行代码的并行化移植.
\item OpenMP : 在要并行的代码块前加预处理指令\#pragma.编译器编译时则自动会链接OpenMP库,编译出并行化代码.
\end{itemize} 
~\n
我们借鉴了OpenMP的并行化的技术,提出了使用外部解释器思想.我们是针对Inferno的shell实现并行计算框架,而shell并不需要编译,是通过Inferno中自带的sh逐行解释执行的.然而直接对Inferno中的sh动手脚是不可能的,sh部分代码量高达好几万行,没有注释并且还使用了部分内核接口.我们之前调研发现Inferno的shell本身就自带bind, cpu, file2chan等工具,可以直接编写能在Inferno的shell上运行的并行化代码.所以我们决定实现一个外部解释器,将用户的串行代码按照给定的需求解释成能直接在Inferno的shell中直接运行的并行化代码.\n

这点和OpenMP又有极大的不同,OpenMP并行化的程序在运行时还需要调用OpenMP库.而我们则要实现串行代码再经过解释器解释之后可以直接在Inferno的sh上运行,不需要再调用其他库,可以说是得到了在Inferno平台下“完全并行化”的代码.

\subsubsection{以C语言而非Limbo语言来编写解释器程序}
在Inferno里，我们需要用Limbo来写应用程序.Limbo是一种用于分布式系统的编程语言.然而Limbo是一门不流行的语言，相关文档较少，因此学习这门语言势必需要花费较多的时间与精力.但是，假如我们是以应用程序的形式在别的系统上运行Inferno，那么我们就可以通过os这条命令，来间接地在Inferno上运行host机器上的程序.于是，我们小组选择了用C语言来写解释器，通过os命令来间接地让解释器在Inferno上运行，大大加快了开发进度.

\subsubsection{对file2chan的读写方法封装,实现一个文件完成各种类型通讯}
此部分具体见概要设计报告中的sharedPool - 节点间通讯一节. 


\section{概要设计报告}

\subsection{总体路线}
在理论依据部分和创新点部分已经提及了我们的部分设计思路.我们的最终目的就是实现一个外部解释器,可以根据需求将用户给定的串行程序翻译成基于host~\&~device模式的,可以在Inferno的shell环境下直接运行的并行化代码. \n
总的开发阶段可以划分为如下两个部分:
\begin{itemize}
\item 自己先摸索出在Inferno中将串行程序并行化改写的方法,并对具体语句总结出一套模式化改写规则(如对一段for改如何改写)
\item 开发解释器,按照之前总结的改写规则将串行代码翻译为并行代码.
\item 在Inferno在x86/64的机子上完成部署．并将Inferno移植到Raspberry Pi上，搭建通过局域网互联的跨架构的集群．
\item 在搭建的集群上测试我们解释器，完成最终部署．
\end{itemize}
~\n

\subsection{平台模型 - OpenCL的host~\&~device模式}
根据我们之前对常见并行计算框架的调研以及Inferno的架构，我们最终决定选用Opencl所用的平台模型 - host~\&~device模型作为我们的平台模型. 原因如下 : 
\begin{itemize}
\item host~\&~device模式非常成熟．当前最为流行的支持异构的并行计算框架OpenCL和CUDA都采用它作为自己的平台模型
\item 在host~\&~device模式中，host就像一个大管家，完成任务调度，节点通讯，负载均衡功能．因此device则只需care自己那部分的运算即可，无需care自身目前所处的环境．即实际运行环境对device是不可见的，device不需要对运行环境的变化做出变化．这一点对于异构是至为重要的．
\item Inferno的架构非常适合采用host~\&~device模式．根据前期对Inferno的namespace和styx的调研，我们可以简单的用bind的命令将远程的Inferno节点的资源挂载在本地,亦或是用cpu命令直接在远程端运行给定程序．所以可以选择一台节点作为host,其余节点作为device.然后将device全部挂载在host上.由host分发任务,响应device请求,进行device间调度等.采用这种平台模型可以大大推进我们开发速度.
\end{itemize}

\subsection{异构性}
首先，我们采用了OpenCL和CUDA所使用的平台模型 - host~\&~device模型作为我们的平台模型，奠定了异构基础．\n
为了进一步支持，我们提出了细并行粒度和抢占式任务分配的观点．

\subsubsection{抢占式任务分配}
为了更好地支持异构，我们决定采用抢占式任务分配的方式．在这种方式下，host无需主动分配任务给device，而是将下一个任务放在共享池中让已经完成了之前任务的空闲device来抢占．当前共享池中的任务被抢占后，host根据计算情况再将新的任务放在共享池中．

\subsubsection{细并行粒度}
在并行计算中，并行粒度 = 计算量 / 通信量。计算和通信交替出现，两者通过同步时间相区分。并行粒度受限于应用程序算法的内在特性。由于选择正确的并行粒度有助于程序暴露更多的可并行性，因此为了充分发挥基础平台的性能，选择正确的并行粒度非常重要．选择合适的并行粒度可以提高并行化加速比。经过调研我们得到如下结论 : 

\paragraph{细粒度的并行}
\begin{itemize}
\item 计算强度低
\item 没有足够的任务来隐藏长时间的异步通信
\item 容易通过提供大量可管理的(即更小的)工作单元来实现负载均衡
\end{itemize}

\paragraph{粗粒度的并行}
\begin{itemize}
\item 计算强度高
\item 完整的应用可以作为并行的粒度
\item 难以有效实现负载均衡
\end{itemize}

根据上述结论，我们决定采用细并行粒度．对于每个device，由于并行粒度细，各个device之间同步需求不大，且每次由host分配给它的任务并不繁重，可以在较短时间计算完成．再由抢占式任务分配机制，计算完成的节点把结果提交给host之后可以直接从host中抢占任务，而不是等待其余device.　\n
如此实现，在一定程度上可以避免计算能力低的节点拖累计算能力高的节点，从而提高了异构性．

\subsection{Raspberry Pi2}
Inferno强调的是分布式，适用于多种架构，从pc到嵌入式系统.所以在设计并行框架时应该利用Inferno这一强大的特性实现对异构的支持.于是搭建Inferno集群时，我们准备安排一个arm架构的结点．\n
\begin{figure}[htbp]
\centering
\includegraphics[width=4.8in,height=2.3in]{pic/raspberry-pi.jpg}
\caption{Raspberry Pi2}
\end{figure}

经过调研论证，我们决定选择Raspberry Pi2作为arm节点．\n
Raspberry Pi是一款基于Linux系统的只有一张信用卡大小的单板机电脑。它由英国的树莓派基金会所开发，目的是以低价硬件及自由软件刺激在学校的基本的电脑科学教育。\n
2015年2月，树莓派基金会发布了第二代产品 - Raspberry Pi2，售价35美元。Raspberry Pi2采用4核Broadcom BCM2836 (ARMv7-A)芯片、双核VideoCore IV GPU和1GB内存，\n

\section{实践部分}
纸上得来终觉浅，绝知此事要躬行．我们组经过反复论证探讨之后，已经开始初步的实践工作．

\subsection{Inferno在Raspberry Pi移植}
根据前期调研，我们决定采用hosted方式在Raspberry Pi上运行inferno．刚好组长阮震元所在的ACSA实验室里有空闲的树莓派可以使用．\n
首先我们在自己的电脑上搭建了arm的交叉编译工具链条．根据Raspberry Pi的情况我们采用了armv6j-hardfloat-linux-gnueabi.完成了arm-glibc, arm-gcc等交叉编译环境的部署．　\n
接着即为针对Raspberry Pi的环境对inferno源码做了11处修改，使得原版的Inferno与Raspberry Pi配适. \n
进行的过程中遇到了各种困难，例如Inferno原版的setfcr-Linux-arm.c已经过时了,在使用4.7以上的arm-gcc会遇到错误，需要对其根据编译环境做相应调整，又如Inferno的styx协议无法在Raspberry Pi上正常运行，最终定位到内核AF\_INET模块，需要对内核配置进行修改. \n
 在移植的过程中，我们还积极联系了Inferno的官方开发组，和他们共同完成这项工作．在此阶段组长阮震元还向Inferno官方上报了2个Inferno在arm下的bug，均引起重视．一个被官方列为major等级\footnote{https://bitbucket.org/inferno-os/inferno-os/issue/326/arm-listen-failed-because-ipv6-support-is},另一个则被官方列为critical等级\footnote{https://bitbucket.org/inferno-os/inferno-os/issue/325/arm-build-failed}.阮震元还针对Inferno在amd64下编译问题向官方提供了一个有效的patch\footnote{https://bitbucket.org/inferno-os/inferno-os/issue/329/a-patch-for-x64}.\n
最终我们在Raspberry Pi上成功运行了Inferno!
\subsection{一个串行程序}
\lstinputlisting{code/deviceSerial.sh}
~\n
先简要的解释一下这个程序.subfn fac实现了一个计算阶乘的子函数.subfn c实现了一个组合数的子函数.subfn calc实现了一个统计函数,对于给定的n可以统计有多少个$r\leq n$满足$C(n,r) > 1000000$.\n
最外层则是一个i从23到100变化的for循环,将所有的calc(i)相加输出.值得注意的是在Inferno下任何算数运算都采用后缀表达式并通过调用内嵌子函数expr完成.\n
本身这个程序没有任何意义,举这个程序为例子只是为了演示如何将一段串行代码并行化.

\subsection{改写的host~\&~device式并行代码}
\paragraph{host部分}  
\lstinputlisting{code/host.sh}
\paragraph{device部分}
\lstinputlisting{code/device.sh}  

\subsection{file2chan - sharedPool的基础}
file2chan是user namespace下的程序，通过对Inferno内核的系统调用实现了内存中的虚拟文件。\n
file2chan接受2个代码块writecmd和readcmd作为参数。当用户读虚拟文件时则会执行readcmd中的代码，当用户对虚拟文件写时则会执行writecmd中的代码。\n
利用file2chan我们将虚拟出一个虚拟文件并进一步封装成一个sharedPool用来做动态的节点间进程通讯。\n
file2chan的代码如下
\lstinputlisting{code/file2chan.b}


\subsection{sharedPool - 节点间通讯(任务分配收发池)}
首先在host部分我们通过file2chan定义了一个虚拟文件/tmp/sharedPool用作节点间通讯.然而由于节点间通讯消息种类十分繁杂(例如主节点给从节点分配任务,从节点计算完毕将结果提交给主节点),各自的格式也都不一样.如果开多个file2chan则必定会带来管理的紊乱.于是我们创新性的采用封装了file2chan的read,write方法使得一个file2chan可以完成各种类型的信息通讯. \n
当想使用sharedPool进行消息通讯时,首先向sharedPool写入具体的操作类型.sharedPool中的我写好的read的方法会读取给定的操作类型,并在内部根据给定操作类型切换响应的模式来应对. \n
例如在我这个例子中,当device想要向host索求数据时先向sharedPool写入get,然后再对sharedPool执行读取操作即可读到主节点分配给它数据.当device计算完成,要把结果反馈给host时就先向sharedPool写入put,然后再往sharedPool写入算得的数据,此时就会触发sharedPool的write方法,自动将本次device向host提交的结果计入总结果.

\subsection{device部分代码}
并行部分的device代码和之前的deviceSerial的主要区别就是任务的分配方式不同。串行版的deviceSerial是自己决定计算任务，而并行版的device是向host抢占任务，待抢占到的任务计算完毕后则将结果放回sharedPool,而device则无需关心结果该如何统计这一切都由host完成，device只需再向host抢占任务即可。\n
由于已经对sharedPool做了高度封装，device只需简单调用即可。抢占任务由subfn getTask实现，提交结果由subfn writeRequest实现。

\subsection{host部分代码}
host部分主要实现了对sharedPool的封装。fn checkHost和fn readyHost负责启动各个device节点，使其进入pending状态。file2chan /tmp/sharedPool实现了对sharedPool的封装，内部的lock段代码再结合外部的subfn cannotWrite实现了对sharedPool的同步处理。

\subsection{节点间通讯同步}
Inferno本身已经对file2chan做了同步处理,当你往里写时就不能读,读时就不能写.一个节点在读的同时其他节点就不能读,一个节点在写时其他节点就不能写.我们唯一要做的是保持事务的原子性.因为我们之前封装了file2chan的read,write方法使得其支持各种类型的通讯信息,这带来方便也带来了弊端.例如一个节点往sharedPool中写入了get,于是sharedPool内部切换为get模式等待这个节点来取数据.然而若在此时另外一个节点计算完了数据想向sharedPool提交数据,于是在之前那个节点读数据之前又往sharedPool写入了put,使得其内部转换为了put模式.那么之前那个节点再读数据就会发生错误.我们要做的就是使得一个节点向sharedPool指明操作类型之后的下一步只接受这个节点的请求(即事务的原子性). \n
我们的实现方法是当sharedPool接收到a节点指定的命令类型之后,马上锁定a节点.下一步只接受来自a节点的请求.如果b节点此时再往sharedPool写入就会无效.然而这还有一问题就是要让b节点等待一会再执行写入操作,而不是直接跳过这次写入操作,否则会导致信息丢失.如果file2chan可以提供write方法的返回值,通过告诉节点b执行write方法的成功与否来让他继续执行或等待.\n
可惜的是Inferno中没有提供对file2chan中read,write方法返回值的支持.于是我们创新性的采用文件名的方式来解决这个问题.由于每个节点都对应一个进程号pid,当pid为x的节点写操作成功时就在/tmp目录下生成一个名为x的文件.当pid为x的进程检测到目录中有x这个文件时就知道了它本次write操作是成功的,可以删掉这个文件(消除痕迹)继续往下执行,否则等待.通过方式我们绕开了Inferno中file2chan的限制,成功的实现了同步效果.

\subsection{实测 - Rasberry Pi与Macbook并行执行}
为了便于调试,我们将host和device的stdout都重定向到了日志文件.执行环境是一台Macbook和Raspberry Pi(在一个局域网内),Macbook充当主节点,Raspberry充当从节点.由于已经封装好代码，在Macbook上Inferno中执行cpu host.sh 192.168.1.1(局域网中Macbook的IP) 192.168.1.10(局域网中的Raspberry IP)便可实现两个节点对跑. \n
运行完毕后/pool目录中有3个日志文件.\n
\paragraph{raspberrypi.log}
\lstinputlisting{log/raspberrypi.log}
\paragraph{macbook.log} 
\lstinputlisting{log/macbook.log}
\paragraph{host.log}
\lstinputlisting{log/host.log}
~\n 可以看到Macbook(pid 45)和Raspberry Pi(pid 622)并行的完成了整个任务的计算. \n
同时我还比较了串行程序运行时间与两个节点并行化的运行时间.对于这个例子,串行时间为1分32秒,两个节点并行化运行时间为1分10秒,可以看到加速比为1.32.加速比不大可以从log中反应出来，因为树莓派性能有限，执行的任务较少。 \n


\subsection{Summary}
虽然目前只是在2个节点上运行，但是可以在一定程度上说明其对异构的支持。Macbook和Raspberry Pi的计算能力跨度很大，然而在我们的并行代码下没有出现慢节点拖累快节点的现象，而是让慢节点尽可能的贡献自己的力量，“能算多少算多少”。结合Inferno的分布式，这将具有重大的意义。并行计算不再要求节点架构一致，操作系统一致(只需在各自操作系统上以hosted模式运行Inferno即可)，计算能力相似等苛刻要求。一个任意的闲置的机子，可以是手机，arm开发板，笔记本电脑，高性能服务器，都可以充当Inferno下的集群节点，加速整个集群计算，做到“物尽其用”。\n
现在我们还停留在人工编写并行代码的阶段。等到我们逐渐深入，对整个架构更加明确时便可开始实现对解释器的编写。让不知道实现并行细节的普通用户用解释器将他们的串行代码翻译成Inferno下的并行代码执行。
\end{document}
 
%%% Local Variables:
%%% mode: latex
%%% TeX-host: t
%%% TeX-master: t
%%% End: 

